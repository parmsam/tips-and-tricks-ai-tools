# Considerations

Establishing guardrails for the effective and responsible use of AI assistance tools is essential. This section outlines key considerations, including the need to comprehend the outputs these tools generate and the significance of using separators in prompt crafting. By delving into these elements, the framework aims to equip you with the means to both optimize the utility of AI assistance tools and safeguard their ethical and precise application in diverse tasks.

## Understanding

AI assistants shouldn't replace good understanding of your code and selected programming language. Developers should be aware of [common failures](https://arxiv.org/abs/2302.03494) of systems they may be using. You should also be cautious about use of tools like [ChatGPT to avoid learning](https://youtu.be/IgxzcOugvEI?t=214).

## Sensitive data

Ensure that you don't provide AI code assistants such as ChatGPT or Github CoPilot with sensitive code or data. There have been many cases of [employees pasting sensitive data](https://www.darkreading.com/risk/employees-feeding-sensitive-business-data-chatgpt-raising-security-fears), with some [estimates of how common this is](https://www.cyberhaven.com/blog/4-2-of-workers-have-pasted-company-data-into-chatgpt/). Consider reading the privacy policy for the AI assistant that you're using and ensure sensitive data is kept separate from such systems.

## Plagiarism

Consider risk of unintentional plagiarism or copyright violation associated with overuse of tools like ChatGPT. The [story of sumplete](https://www.digitaltrends.com/gaming/sumplete-chatgpt-ai-game-design-ethics/), [numerous ongoing lawsuits](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data), and numerous [ethical issues](https://arxiv.org/abs/2305.10646) show us that these tools should be used with care.

## Trust by verify

Just because something sounds right, doesn't mean it's right. Sometimes AI tools provide incorrect answers, so it's your responsibility to check the results, as recommended [in Nature](https://www.nature.com/articles/d41586-023-01833-0). Check if there are any bugs or hallucinations (such as functions that don't exist) in code you obtain and confirm you understand what each lines is doing. For example, ChatGPT can often generate URLs that don't exist. You can use AI tools to support this verification process but you should be an active part of it. Use unit testing and [assertive programming](https://www.milesmcbain.com/posts/assertive-programming-for-pipelines/) to ensure results perform as expected in the cases you need to cover.

## Iterate on it

As noted [in Nature](https://www.nature.com/articles/d41586-023-01833-0), Sometimes you'll need to have a conversation with an AI tool to get what you want (this is called [prompt chaining](https://www.voiceflow.com/blog/prompt-chaining-conversational-ai)). You might even need to start all over again. It often doesn't work on the first try. Try changing your approach. Consider changing settings for the tool you're using like [temperature in ChatGPT](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#:~:text=particular%20so%2Dcalled%20%E2%80%9C-,temperature,-%E2%80%9D%20parameter%20that%20determines). Experiment with different prompts. Try using different tools.

## List out multiple possibilities

Ask the platform to list out multiple different options. You can specify how many results you want to be generated. You can serve as the curator deciding what is useful vs. throwaway information.

## Use separators

Consider using separators like quotation marks (`""` or `''`) or backticks (\`\`) to separate sections of text that should get treated differently in the prompt, as indicated from [the-decoder](https://the-decoder.com/chatgpt-guide-prompt-strategies/).

## Summarize the conversation

Ask the AI tools to summarize the conversation at completion. This can help recap the tasks accomplished and reflect on what could have been done better.

## Keep it simple

Break your problem into small pieces or tasks, as suggested [in Nature](https://www.nature.com/articles/d41586-023-01833-0). This is similar to the [strategy you should use](https://www.indeed.com/career-advice/career-development/how-to-break-down-project-into-tasks) when working on any complex project. Keeping it simple is a good rule of thumb here. As mentioned before, [*prompt chaining* or *chained prompting*](https://www.voiceflow.com/blog/prompt-chaining-conversational-ai) can be used to solve more complex problems.

## Be specific

Be a good designer and keep your prompts specific and clear. As part of that, you can include the response format or an example response you expect to receive. You could add a word limit or readability level (keep responses at a middle school level for example). You could mention the target audience, desired tone, or a [persona](https://www.howtogeek.com/881659/how-to-create-chatgpt-personas-for-every-occasion/) the tool should adopt. There are a lot of these different *response settings* you can define to mold the responses. Some experimentation is required here, but that's the fun part.

## Be familiar with your tool

Learn how your AI tool's features and any settings you can change. For example, Github Copilot uses a feature called neighboring tabs to contextualize recommendations according to other tabs open in the IDE. Therefore, you may need to keep tabs open with material that may be relevant to current work for the tool. You can also tweak keyboard shortcuts or other settings for tools Github Copilot. Customize these tools to your needs.

### Be aware of limitations

You should consider the training cutoff dates for the LLM models that you are using. The training cutoff date for OpenAI's GPT-3.5 and GPT-4 models is September 2021. This may limit the ability include more recent information in chat results.  Other platforms like Bing and Bard can have more recent results since they can search the internet. 

Also be aware that [many of these LLMs do not disclose](https://simonwillison.net/2023/Aug/27/wordcamp-llms/#how-they-are-trained) the training data used. This makes it unclear what it is probably good at answer and isn't good at answering out of box. 

You can augment ChatGPT via [various plugins](https://openai.com/blog/chatgpt-plugins) that are available such as one for [Wolfram Alpha](https://www.wolfram.com/wolfram-plugin-chatgpt/) or [Wikipedia](https://diff.wikimedia.org/2023/07/13/exploring-paths-for-the-future-of-free-knowledge-new-wikipedia-chatgpt-plugin-leveraging-rich-media-social-apps-and-other-experiments/).

### Beware of prompt injections

Prompt injections are ways of crafting prompts that violate safeguards. They can be used to return biased, malicious, or sensitive information. This is a [known issue](https://www.wired.com/story/chatgpt-prompt-injection-attack-security/) with older versions of ChatGPT. You should be aware of this risk when creating apps that use LLMs.

## Adhere to Simon Willison's Personal AI Ethics

Simon Willison had a great talk in August 2023 on [Making Large Language Models work for you](https://simonwillison.net/2023/Aug/27/wordcamp-llms/). In the talk, he shared his personal AI ethical guidelines which included the following (watch [the talk](https://www.youtube.com/watch?v=LpDulftMkew&t=29900s&ab_channel=WordPress) to learn why):

- Don't publish anything that will take someone longer to read than it took you to write. 
- Never commit code you can't understand and explain every line of.
- Share your prompts and help spread knowledge of how to use these tools
- Be aware of the harm of these AI tools to believably mimic humans. Instead of responses that mimic humans, ask tools like ChatGPT to be a sentient cheesecake or a Shakespearean coal miner. The responses are safer, more entertaining, and stick better.

## Summary

In this chapter, we explored a variety of suggestions when using AI code assistants. We covered the importance of understanding, sensitive data, plagiarism, trust but verify, iterating, listing out multiple possibilities, and many other helpful principles. I hope you found this chapter helpful and that you'll be able to use these principles to more safely use AI assistance tools.
